{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import MBartModel, MBart50Tokenizer\n",
    "\n",
    "model_path = \"models/\"\n",
    "\n",
    "sl_orig = \"en_XX\"\n",
    "\n",
    "sls = [\"en_XX\", \"ta_IN\", \"xh_ZA\", \"vi_VN\"]\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "limit = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_embeddings(method, data, tokenized, pad_tok_id):\n",
    "  if \"attention_mask\" in tokenized:\n",
    "    attention_mask = tokenized[\"attention_mask\"]\n",
    "  else: # apparently ErnieM does NOT have attenion IDs in the tokenized output, so I am \"computing\" them myself - like in all other models, the model should not pay attention to [PAD] tokens, so they are ignored/not paid attention to\n",
    "    token_ids = tokenized[\"input_ids\"][0]\n",
    "    padding_ids = len([tok for tok in token_ids if tok == pad_tok_id]) # count how many [PAD] tokens there are\n",
    "    attention_mask = torch.ones((tokenized[\"input_ids\"].shape)).to(device)\n",
    "    if padding_ids > 0:\n",
    "        attention_mask[:,-padding_ids:] = 0\n",
    "    attention_mask = torch.tensor(attention_mask).to(device)\n",
    "    \n",
    "  attention_expanded = attention_mask.unsqueeze(-1).expand(data.size()).float()\n",
    "  data_attention = data * attention_expanded\n",
    "  return torch.sum(data_attention, 1) / torch.clamp(attention_expanded.sum(1), min=1e-9) # to not divide by 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text, sl):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    inputs = inputs.to(device)\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    pad_tok_id = tokenizer(\"[PAD]\")\n",
    "    pad_tok_id = pad_tok_id[\"input_ids\"][1]\n",
    "\n",
    "    outputs = pool_embeddings(torch.mean, outputs[0], inputs, pad_tok_id)[0]\n",
    "\n",
    "    return np.array(outputs.cpu().detach().numpy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# for sl in sls:\n",
    "# \tif sl == sl_orig:\n",
    "# \t\tcontinue\n",
    "# \tdf = pd.read_csv(f\"snopes_backtranslation_{sl}-{sl_orig}.csv\", sep=\",\")\n",
    "# \tdf = df.replace([True, False], [1, 0])\n",
    "# \tdf.head()\n",
    "\n",
    "# \ttokenizer = MBart50Tokenizer.from_pretrained(model_path + \"mbart-large-50-many-to-many-mmt\", src_lang=sl)\n",
    "# \tmodel = MBartModel.from_pretrained(model_path + \"mbart-large-50-many-to-many-mmt\").to(device)\n",
    "\n",
    "# \ttexts = df[\"claim\"].iloc[:limit]\n",
    "# \tlabels = df[\"label\"].iloc[:limit]\n",
    "\n",
    "# \tembds = []\n",
    "# \tfor t in tqdm(texts):\n",
    "# \t\tembds.append(get_embeddings(t, sl))\n",
    "\n",
    "# \tembds = np.asarray(embds)\n",
    "# \tnp.save(f\"emb_{sl}_back.npy\", np.asarray(embds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3324, 1024) (1109, 1024)\n",
      "[[941   1]\n",
      " [167   0]]\n",
      "(3324, 1024) (1109, 1024)\n",
      "[[942   0]\n",
      " [167   0]]\n",
      "(3324, 1024) (1109, 1024)\n",
      "[[942   0]\n",
      " [167   0]]\n",
      "(3324, 1024) (1109, 1024)\n",
      "[[942   0]\n",
      " [167   0]]\n"
     ]
    }
   ],
   "source": [
    "results_svm_ph1 = []\n",
    "for sl in sls:\n",
    "\tif sl != sl_orig:\n",
    "\t\tdf = pd.read_csv(f\"snopes_{sl}.csv\")\n",
    "\telse:\n",
    "\t\tdf = pd.read_csv(f\"cleaned_snopes.csv\", sep=\"|\")\n",
    "\tdf = df.replace([True, False], [1, 0])\n",
    "\n",
    "\tlabels = df[\"label\"]\n",
    "\n",
    "\tdf.head()\n",
    "\tembds = np.load(f\"emb_{sl}.npy\")\n",
    "\n",
    "\tx_train, x_test, y_train, y_test = train_test_split(embds, labels, test_size=0.25, random_state=42)\n",
    "\tprint(x_train.shape, x_test.shape)\n",
    "\n",
    "\tsvm = SVC()\n",
    "\tsvm.fit(x_train, y_train)\n",
    "\tpred = svm.predict(x_test)\n",
    "\n",
    "\tconf_matrix = confusion_matrix(y_test, pred)\n",
    "\tresults_svm_ph1.append(conf_matrix)\n",
    "\n",
    "\tprint(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3324, 1024) (1109, 1024)\n",
      "[[942   0]\n",
      " [167   0]]\n",
      "(3324, 1024) (1109, 1024)\n",
      "[[942   0]\n",
      " [167   0]]\n",
      "(3324, 1024) (1109, 1024)\n",
      "[[942   0]\n",
      " [167   0]]\n"
     ]
    }
   ],
   "source": [
    "results_svm_ph2 = []\n",
    "for sl in sls:\n",
    "\tif sl == sl_orig:\n",
    "\t\tcontinue\n",
    "\tdf = pd.read_csv(f\"snopes_backtranslation_{sl}-{sl_orig}.csv\", sep=\",\")\n",
    "\tdf = df.replace([True, False], [1, 0])\n",
    "\tdf.head()\n",
    "\n",
    "\tlabels = df[\"label\"].iloc[:limit]\n",
    "\t\n",
    "\tembds = np.asarray(embds)\n",
    "\tnp.save(f\"emb_{sl}_back.npy\", np.asarray(embds))\n",
    "\n",
    "\tx_train, x_test, y_train, y_test = train_test_split(embds, labels, test_size=0.25, random_state=42)\n",
    "\tprint(x_train.shape, x_test.shape)\n",
    "\n",
    "\tsvm = SVC()\n",
    "\tsvm.fit(x_train, y_train)\n",
    "\tpred = svm.predict(x_test)\n",
    "\n",
    "\tconf_matrix = confusion_matrix(y_test, pred)\n",
    "\tresults_svm_ph2.append(conf_matrix)\n",
    "\n",
    "\tprint(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3324, 1024) (1109, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/intel/icelake/software/SciPy-bundle/2022.05-foss-2022a/lib/python3.10/site-packages/scipy/optimize/_linesearch.py:305: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home2/s4231317/virtual_env/HC/lib/python3.10/site-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[861  81]\n",
      " [135  32]]\n",
      "(3324, 1024) (1109, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/intel/icelake/software/SciPy-bundle/2022.05-foss-2022a/lib/python3.10/site-packages/scipy/optimize/_linesearch.py:305: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home2/s4231317/virtual_env/HC/lib/python3.10/site-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[858  84]\n",
      " [144  23]]\n",
      "(3324, 1024) (1109, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/intel/icelake/software/SciPy-bundle/2022.05-foss-2022a/lib/python3.10/site-packages/scipy/optimize/_linesearch.py:305: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home2/s4231317/virtual_env/HC/lib/python3.10/site-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[859  83]\n",
      " [138  29]]\n",
      "(3324, 1024) (1109, 1024)\n",
      "[[855  87]\n",
      " [135  32]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/intel/icelake/software/SciPy-bundle/2022.05-foss-2022a/lib/python3.10/site-packages/scipy/optimize/_linesearch.py:305: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home2/s4231317/virtual_env/HC/lib/python3.10/site-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "results_lr_ph1 = []\n",
    "for sl in sls:\n",
    "\tif sl != sl_orig:\n",
    "\t\tdf = pd.read_csv(f\"snopes_{sl}.csv\")\n",
    "\telse:\n",
    "\t\tdf = pd.read_csv(f\"cleaned_snopes.csv\", sep=\"|\")\n",
    "\tdf = df.replace([True, False], [1, 0])\n",
    "\n",
    "\tlabels = df[\"label\"]\n",
    "\n",
    "\tdf.head()\n",
    "\tembds = np.load(f\"emb_{sl}.npy\", )\n",
    "\n",
    "\tx_train, x_test, y_train, y_test = train_test_split(embds, labels, test_size=0.25, random_state=42)\n",
    "\tprint(x_train.shape, x_test.shape)\n",
    "\n",
    "\tlr = LogisticRegression(solver=\"newton-cg\")\n",
    "\tlr.fit(x_train, y_train)\n",
    "\tpred = lr.predict(x_test)\n",
    "\n",
    "\tconf_matrix = confusion_matrix(y_test, pred)\n",
    "\tresults_lr_ph1.append(conf_matrix)\n",
    "\n",
    "\tprint(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3324, 1024) (1109, 1024)\n",
      "int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/intel/icelake/software/SciPy-bundle/2022.05-foss-2022a/lib/python3.10/site-packages/scipy/optimize/_linesearch.py:305: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home2/s4231317/virtual_env/HC/lib/python3.10/site-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[861  81]\n",
      " [135  32]]\n",
      "(3324, 1024) (1109, 1024)\n",
      "int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/intel/icelake/software/SciPy-bundle/2022.05-foss-2022a/lib/python3.10/site-packages/scipy/optimize/_linesearch.py:305: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home2/s4231317/virtual_env/HC/lib/python3.10/site-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[861  81]\n",
      " [135  32]]\n",
      "(3324, 1024) (1109, 1024)\n",
      "int64\n",
      "[[861  81]\n",
      " [135  32]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/intel/icelake/software/SciPy-bundle/2022.05-foss-2022a/lib/python3.10/site-packages/scipy/optimize/_linesearch.py:305: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home2/s4231317/virtual_env/HC/lib/python3.10/site-packages/sklearn/utils/optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n"
     ]
    }
   ],
   "source": [
    "results_lr_ph2 = []\n",
    "for sl in sls:\n",
    "\tif sl == sl_orig:\n",
    "\t\tcontinue\n",
    "\tdf = pd.read_csv(f\"snopes_backtranslation_{sl}-{sl_orig}.csv\", sep=\",\")\n",
    "\tdf = df.replace([True, False], [1, 0])\n",
    "\tdf.head()\n",
    "\n",
    "\tlabels = df[\"label\"].iloc[:limit]\n",
    "\t\n",
    "\tembds = np.asarray(embds)\n",
    "\tnp.save(f\"emb_{sl}_back.npy\", np.asarray(embds))\n",
    "\n",
    "\tx_train, x_test, y_train, y_test = train_test_split(embds, labels, test_size=0.25, random_state=42)\n",
    "\tprint(x_train.shape, x_test.shape)\n",
    "\n",
    "\tlr = LogisticRegression(solver=\"newton-cg\")\n",
    "\tlr.fit(x_train, y_train)\n",
    "\tpred = lr.predict(x_test)\n",
    "\n",
    "\tconf_matrix = confusion_matrix(y_test, pred)\n",
    "\tresults_lr_ph2.append(conf_matrix)\n",
    "\n",
    "\tprint(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[941,   1],\n",
      "       [167,   0]]), array([[942,   0],\n",
      "       [167,   0]]), array([[942,   0],\n",
      "       [167,   0]]), array([[942,   0],\n",
      "       [167,   0]])]\n",
      "[array([[942,   0],\n",
      "       [167,   0]]), array([[942,   0],\n",
      "       [167,   0]]), array([[942,   0],\n",
      "       [167,   0]])]\n"
     ]
    }
   ],
   "source": [
    "print(results_ph1)\n",
    "print(results_ph2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
